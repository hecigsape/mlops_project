from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count

def create_spark_session():
    """
    Crea una sesiÃ³n de Spark.
    """
    return SparkSession.builder.appName("AmazonSoftwareReviews").getOrCreate()

def load_jsonl(file_path, spark):
    """
    Carga el dataset JSONL en un DataFrame de Spark.
    """
    print(f"ğŸ“¥ Cargando datos desde {file_path}...")
    df = spark.read.json(file_path)
    print("âœ… Datos cargados correctamente.")
    return df

def explore_data(df):
    """
    Explora la estructura y contenido del dataset.
    """
    print("\nğŸ“Œ Esquema del dataset:")
    df.printSchema()
    
    print("\nğŸ“Š Cantidad total de registros:")
    print(f"âœ… Total de registros: {df.count()}")
    
    print("\nğŸ” VerificaciÃ³n de valores nulos:")
    df.select([count(col(c)).alias(c) for c in df.columns]).show()

def save_as_parquet(df, output_path):
    """
    Guarda el DataFrame en formato Parquet para mayor eficiencia.
    """
    print(f"\nğŸ’¾ Guardando datos en {output_path}...")
    df.write.mode("overwrite").parquet(output_path)
    print("âœ… Datos guardados correctamente.")

if __name__ == "__main__":
    # ğŸ“Œ Definir rutas
    raw_data_path = "data/raw/Software.jsonl"
    interim_data_path = "data/interim/Software_interim.parquet"

    # Crear sesiÃ³n de Spark
    spark = create_spark_session()

    #  Cargar datos
    df = load_jsonl(raw_data_path, spark)

    #  Explorar los datos
    explore_data(df)

    # Guardar en Parquet
    save_as_parquet(df, interim_data_path)

    # FinalizaciÃ³n
    print("\nğŸ¯ Proceso de ingestion completado.")
